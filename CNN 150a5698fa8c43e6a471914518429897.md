# CNN

- CNN이란
    
    **CNN(Convolutional Neural Network, 컨볼루션 신경망)**
    
    주로 시각적 이미지를 분석하는 데 사용되는데 머신러닝의 한 유형의 딥러닝
    
    CNN은 이미지를 인식하기 위해 패턴을 찾는 데 유용하다. 
    
    데이터를 통해 특징을 스스로 학습하고, 패턴을 사용하여 이미지를 분류하고 특징을 수동으로 추출할 필요가 없다.
    
    기존 네트워크 바탕으로 새로운 인식 작업을 위해 CNN을 재학습하여 사용하는 것이 가능하다.
    
- CNN의 구조와 구성요소
    1. Convolutional Layer(합성곱층)
        - 필터 (filter)
            - Filter를 통해 특징을 추출할 수 있고 이 추출한 특징을 Feature map 이라고 한다.
        - Stride
            - 필터를 원본이미지에 컨볼루션할 때, 얼마만큼의 간격으로 진행할지에 대한 값을 stride 라고 한다.
        - Padding
            - 필터를 적용할 때 결과값이 작아지는데, 이러한 현상을 막기위해 input계층 주위로 특정한 값(예를 들어, 0)으로 둘러쌓는 것을 Padding이라고 한다.  오버피팅을 방지한다.
    2. Activation Function (활성화 함수)
        - 필터를 통해 얻은 Feature map에 활성화 함수(Activation Function)을 적용한다. 활성화 함수의 역할은 Feature map에 특징이 있으면 픽셀은 큰값, 특징이 없는 픽셀은 0에 가까운 값이 담겨있다. 이 값이 정량적인 값으로 나오기 때문에, 이 값을 특징이 “있다 없다”의 비선형 값으로 바꿔주는 과정이 필요한데, 이것이 바로 Activation Function이다. 다양한활성화함수가 있는데 보통 활성화 함수로는 ReLu 함수를 사용한다.
        - ReLu를 주로 사용하는 이유
            - 뉴럴 네트워크에서는 신경망이 깊어질수록 학습이 어렵다는 문제점이 있다. 이를 보완하기 위해 Back propagation(역전파)라는 방법을 사용하는데, 이는 계산한 값을 재활용하여 다시 계산하는 것을 말한다. sigmoid함수의 경우 레이어가 깊어지면 역전파가 제데로 작동하지 안기 때문에 사용한다.
        - 선형함수가 아니라 비선형 함수를 사용하는 이유
            - 비선형 함수를 사용하는 이유는 선형함수를 사용할 시 층을 깊게 하는 의미가 줄어들기 때문이다. 선형함수 f(x)= ax를 3층 네트워크로 쌓았다고 할 때y(x) = f(f(f(x)))가 된다. 이는 사실 y(x) = cx와 같은데, c = a^3 이기 때문이다. 따라서 은닉층의 의미가 없어지기 때문에 층을 쌓는 것을 통해 더 좋은 결과값을 도출해내는 목적을 이루기위해 사용한다.
    3. Pooling Layer(풀링층)
        - 활성화 함수까지 거쳐 나온 Feature Map을 인위로 줄이는 작업을 서브 셈플링, 즉 풀링(Polling)이라고 한다. 풀링 방법은 주로 Max polling을 기반으로 구현되지만, average polling, L2-norm polling이 있다.
        - Max polling
            - Feature map을 M x  N 사이즈로 조각내서, 그 국소 영역마다 가장 큰 값을 뽑아내는 방법이다. 이는 특징의 값이 큰 값이 다른 특징들을 대표한다라는 개념을 기반으로 하고 있다.
    4. Fully Connected Layer (전결합층)
        - Convolution계층과 Polling계층을 통해 추출된 특징 값을 기존의 인공신경망에 넣어 분류를 해야한다. 이 Fully conneted Layer는 기존 인공 신경망의 구조라고 생각하면 된다. 이 계층을 지나면 Dropout 계층과 Softmax Funtion을 거쳐서 Output이 나오게 하다.
    5. Dropout Layer(드롭아웃층)
        - 드롭아웃은 오버피팅을 막기위한 방법으로 뉴럴 네트워크가 학습중일 때, 랜덤하게 뉴련을 꺼서 학습을 방해하는 기법이다. 이를 통해 모델이 학습용 데이터에 과적합(오버피팅)하는 현상을 막아준다
    6. Softmax 함수
        - sigmoid, LeRu 같은 액티베이션 함수의 일종이다. sigmoid가 결과 값에 따라 1, 0 과 같이 이산 분류하는 함수라면, Softmax는 여러개의 분류를 가질 수 있는 함수이다. 이 결과로는 다음과 같은 예시로 나올 수 있다. 카테고리가 자동차, 트럭, 비행기, 기차일 때 어떤 인풋데이터에 대해서
        - 자동차일 확률 0.7
        - 트럭일 확률 0.2
        - 비행기일 확률 0.03
        - 기차일 확률 0.07
        
        과 같이 표시가 된다. 이때 모든 카테고리 확률의 합은 1이되어야 한다.